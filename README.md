# Дистилляция и прунинг модели SegFormer для сегментации людей

Цель — создать компактную модель студента, сохраняющую качество большой учительской модели. После используя код для прунинга модели получить оптимизированую версию SegFormer для задачи сегментации людей. 

* В дистиляции реализованно сжатие модели: Удаление 50% слоёв, что снижает модель студента с ~3.7М до ~1.3М параметров 
* В пругинге используются методы Magnitude Pruning и Taylor Pruning, а также идет дообучение моделей после прунинга.

## Описание проекта

### Методы
1. **Дистилляция**: Уменьшение размера исходной модели (baseline) с сохранением производительности.
    - Инициализация моделей (учитель и ученик)
    - Процесс обучения с разными типами лоссов и сятгиванием слоев ученика к учителю
2. **Прунинг**:
   - **Magnitude Pruning**: Удаление наименее значимых весов на основе L2-нормы. Относится к Uniform прунингу
   - **Taylor Pruning**: Накопление градиентов для оценки важности весов. Удаление наименее полезных  
3. **Дообучение**: Восстановление точности после прунинга с использованием дистилляции.


## Результаты экспериментов

| Метрика                     | Baseline       | После дистилляции | После Magnitude Pruning | После Taylor Pruning  |
|:----------------------------|:--------------:|:-----------------:|:-----------------------:|:---------------------:|
| Параметры (млн)             | 3.71           | 1.30              | 0.73                    | 0.81                  |
| Вычислительная сложность (MMAC) | 6,761       | 5,842             | 4,762                   | 4,794                 |
| Mean IoU                    | 0.986          | 0.978             | 0.920 → 0.956*          | 0.942 → 0.955*        |

<small>*После дообучения</small>
